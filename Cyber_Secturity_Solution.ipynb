{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBfvho0RZSxzKR56+WG78n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DEVANG-2021/ZeroShield---AI-Powered-Adversarial-Defense-for-Zero-Trust-Security/blob/main/Cyber_Secturity_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkF_B7pTnp2l",
        "outputId": "aeb2efec-f398-420a-c357-99636cfe56ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/foolbox/attacks/blur.py:3: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n",
            "<ipython-input-10-359f0a612773>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
            "<ipython-input-10-359f0a612773>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.fillna(df.mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Destination Port              float32\n",
            " Flow Duration                 float32\n",
            " Total Fwd Packets             float32\n",
            " Total Backward Packets        float32\n",
            "Total Length of Fwd Packets    float32\n",
            "                                ...   \n",
            " Active Min                    float32\n",
            "Idle Mean                      float32\n",
            " Idle Std                      float32\n",
            " Idle Max                      float32\n",
            " Idle Min                      float32\n",
            "Length: 78, dtype: object\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0           0.900071    2.500995e-08            0.000621   \n",
            "1           0.903172    9.086947e-07            0.000000   \n",
            "2           0.903189    4.335057e-07            0.000000   \n",
            "3           0.758456    2.834461e-07            0.000000   \n",
            "4           0.900038    2.500995e-08            0.000621   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                 0.000000                     0.000404   \n",
            "1                 0.000517                     0.000202   \n",
            "2                 0.000517                     0.000202   \n",
            "3                 0.000517                     0.000202   \n",
            "4                 0.000000                     0.000404   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                      0.000000                0.000839   \n",
            "1                      0.000002                0.000839   \n",
            "2                      0.000002                0.000839   \n",
            "3                      0.000002                0.000839   \n",
            "4                      0.000000                0.000839   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                 0.06383                 0.004523                     0.0   \n",
            "1                 0.06383                 0.004523                     0.0   \n",
            "2                 0.06383                 0.004523                     0.0   \n",
            "3                 0.06383                 0.004523                     0.0   \n",
            "4                 0.06383                 0.004523                     0.0   \n",
            "\n",
            "   ...   act_data_pkt_fwd   min_seg_size_forward  Active Mean   Active Std  \\\n",
            "0  ...           0.000987                    0.0          0.0          0.0   \n",
            "1  ...           0.000000                    0.0          0.0          0.0   \n",
            "2  ...           0.000000                    0.0          0.0          0.0   \n",
            "3  ...           0.000000                    0.0          0.0          0.0   \n",
            "4  ...           0.000987                    0.0          0.0          0.0   \n",
            "\n",
            "    Active Max   Active Min  Idle Mean   Idle Std   Idle Max   Idle Min  \n",
            "0          0.0          0.0        0.0        0.0        0.0        0.0  \n",
            "1          0.0          0.0        0.0        0.0        0.0        0.0  \n",
            "2          0.0          0.0        0.0        0.0        0.0        0.0  \n",
            "3          0.0          0.0        0.0        0.0        0.0        0.0  \n",
            "4          0.0          0.0        0.0        0.0        0.0        0.0  \n",
            "\n",
            "[5 rows x 78 columns]\n",
            "Generator Start Running...\n",
            "Generator Stop Running...\n",
            "Discriminator Start Running...\n",
            "Discriminator Stop Running...\n",
            "Epoch [1/500] - Total Samples Processed: 1000\n",
            "Epoch [2/500] - Total Samples Processed: 1000\n",
            "Epoch [3/500] - Total Samples Processed: 1000\n",
            "Epoch [4/500] - Total Samples Processed: 1000\n",
            "Epoch [5/500] - Total Samples Processed: 1000\n",
            "Epoch [6/500] - Total Samples Processed: 1000\n",
            "Epoch [7/500] - Total Samples Processed: 1000\n",
            "Epoch [8/500] - Total Samples Processed: 1000\n",
            "Epoch [9/500] - Total Samples Processed: 1000\n",
            "Epoch [10/500] - Total Samples Processed: 1000\n",
            "Epoch [11/500] - Total Samples Processed: 1000\n",
            "Epoch [12/500] - Total Samples Processed: 1000\n",
            "Epoch [13/500] - Total Samples Processed: 1000\n",
            "Epoch [14/500] - Total Samples Processed: 1000\n",
            "Epoch [15/500] - Total Samples Processed: 1000\n",
            "Epoch [16/500] - Total Samples Processed: 1000\n",
            "Epoch [17/500] - Total Samples Processed: 1000\n",
            "Epoch [18/500] - Total Samples Processed: 1000\n",
            "Epoch [19/500] - Total Samples Processed: 1000\n",
            "Epoch [20/500] - Total Samples Processed: 1000\n",
            "Epoch [21/500] - Total Samples Processed: 1000\n",
            "Epoch [22/500] - Total Samples Processed: 1000\n",
            "Epoch [23/500] - Total Samples Processed: 1000\n",
            "Epoch [24/500] - Total Samples Processed: 1000\n",
            "Epoch [25/500] - Total Samples Processed: 1000\n",
            "Epoch [26/500] - Total Samples Processed: 1000\n",
            "Epoch [27/500] - Total Samples Processed: 1000\n",
            "Epoch [28/500] - Total Samples Processed: 1000\n",
            "Epoch [29/500] - Total Samples Processed: 1000\n",
            "Epoch [30/500] - Total Samples Processed: 1000\n",
            "Epoch [31/500] - Total Samples Processed: 1000\n",
            "Epoch [32/500] - Total Samples Processed: 1000\n",
            "Epoch [33/500] - Total Samples Processed: 1000\n",
            "Epoch [34/500] - Total Samples Processed: 1000\n",
            "Epoch [35/500] - Total Samples Processed: 1000\n",
            "Epoch [36/500] - Total Samples Processed: 1000\n",
            "Epoch [37/500] - Total Samples Processed: 1000\n",
            "Epoch [38/500] - Total Samples Processed: 1000\n",
            "Epoch [39/500] - Total Samples Processed: 1000\n",
            "Epoch [40/500] - Total Samples Processed: 1000\n",
            "Epoch [41/500] - Total Samples Processed: 1000\n",
            "Epoch [42/500] - Total Samples Processed: 1000\n",
            "Epoch [43/500] - Total Samples Processed: 1000\n",
            "Epoch [44/500] - Total Samples Processed: 1000\n",
            "Epoch [45/500] - Total Samples Processed: 1000\n",
            "Epoch [46/500] - Total Samples Processed: 1000\n",
            "Epoch [47/500] - Total Samples Processed: 1000\n",
            "Epoch [48/500] - Total Samples Processed: 1000\n",
            "Epoch [49/500] - Total Samples Processed: 1000\n",
            "Epoch [50/500] - Total Samples Processed: 1000\n",
            "Epoch [51/500] - Total Samples Processed: 1000\n",
            "Epoch [52/500] - Total Samples Processed: 1000\n",
            "Epoch [53/500] - Total Samples Processed: 1000\n",
            "Epoch [54/500] - Total Samples Processed: 1000\n",
            "Epoch [55/500] - Total Samples Processed: 1000\n",
            "Epoch [56/500] - Total Samples Processed: 1000\n",
            "Epoch [57/500] - Total Samples Processed: 1000\n",
            "Epoch [58/500] - Total Samples Processed: 1000\n",
            "Epoch [59/500] - Total Samples Processed: 1000\n",
            "Epoch [60/500] - Total Samples Processed: 1000\n",
            "Epoch [61/500] - Total Samples Processed: 1000\n",
            "Epoch [62/500] - Total Samples Processed: 1000\n",
            "Epoch [63/500] - Total Samples Processed: 1000\n",
            "Epoch [64/500] - Total Samples Processed: 1000\n",
            "Epoch [65/500] - Total Samples Processed: 1000\n",
            "Epoch [66/500] - Total Samples Processed: 1000\n",
            "Epoch [67/500] - Total Samples Processed: 1000\n",
            "Epoch [68/500] - Total Samples Processed: 1000\n",
            "Epoch [69/500] - Total Samples Processed: 1000\n",
            "Epoch [70/500] - Total Samples Processed: 1000\n",
            "Epoch [71/500] - Total Samples Processed: 1000\n",
            "Epoch [72/500] - Total Samples Processed: 1000\n",
            "Epoch [73/500] - Total Samples Processed: 1000\n",
            "Epoch [74/500] - Total Samples Processed: 1000\n",
            "Epoch [75/500] - Total Samples Processed: 1000\n",
            "Epoch [76/500] - Total Samples Processed: 1000\n",
            "Epoch [77/500] - Total Samples Processed: 1000\n",
            "Epoch [78/500] - Total Samples Processed: 1000\n",
            "Epoch [79/500] - Total Samples Processed: 1000\n",
            "Epoch [80/500] - Total Samples Processed: 1000\n",
            "Epoch [81/500] - Total Samples Processed: 1000\n",
            "Epoch [82/500] - Total Samples Processed: 1000\n",
            "Epoch [83/500] - Total Samples Processed: 1000\n",
            "Epoch [84/500] - Total Samples Processed: 1000\n",
            "Epoch [85/500] - Total Samples Processed: 1000\n",
            "Epoch [86/500] - Total Samples Processed: 1000\n",
            "Epoch [87/500] - Total Samples Processed: 1000\n",
            "Epoch [88/500] - Total Samples Processed: 1000\n",
            "Epoch [89/500] - Total Samples Processed: 1000\n",
            "Epoch [90/500] - Total Samples Processed: 1000\n",
            "Epoch [91/500] - Total Samples Processed: 1000\n",
            "Epoch [92/500] - Total Samples Processed: 1000\n",
            "Epoch [93/500] - Total Samples Processed: 1000\n",
            "Epoch [94/500] - Total Samples Processed: 1000\n",
            "Epoch [95/500] - Total Samples Processed: 1000\n",
            "Epoch [96/500] - Total Samples Processed: 1000\n",
            "Epoch [97/500] - Total Samples Processed: 1000\n",
            "Epoch [98/500] - Total Samples Processed: 1000\n",
            "Epoch [99/500] - Total Samples Processed: 1000\n",
            "Epoch [100/500] - Total Samples Processed: 1000\n",
            "Epoch [101/500] - Total Samples Processed: 1000\n",
            "Epoch [102/500] - Total Samples Processed: 1000\n",
            "Epoch [103/500] - Total Samples Processed: 1000\n",
            "Epoch [104/500] - Total Samples Processed: 1000\n",
            "Epoch [105/500] - Total Samples Processed: 1000\n",
            "Epoch [106/500] - Total Samples Processed: 1000\n",
            "Epoch [107/500] - Total Samples Processed: 1000\n",
            "Epoch [108/500] - Total Samples Processed: 1000\n",
            "Epoch [109/500] - Total Samples Processed: 1000\n",
            "Epoch [110/500] - Total Samples Processed: 1000\n",
            "Epoch [111/500] - Total Samples Processed: 1000\n",
            "Epoch [112/500] - Total Samples Processed: 1000\n",
            "Epoch [113/500] - Total Samples Processed: 1000\n",
            "Epoch [114/500] - Total Samples Processed: 1000\n",
            "Epoch [115/500] - Total Samples Processed: 1000\n",
            "Epoch [116/500] - Total Samples Processed: 1000\n",
            "Epoch [117/500] - Total Samples Processed: 1000\n",
            "Epoch [118/500] - Total Samples Processed: 1000\n",
            "Epoch [119/500] - Total Samples Processed: 1000\n",
            "Epoch [120/500] - Total Samples Processed: 1000\n",
            "Epoch [121/500] - Total Samples Processed: 1000\n",
            "Epoch [122/500] - Total Samples Processed: 1000\n",
            "Epoch [123/500] - Total Samples Processed: 1000\n",
            "Epoch [124/500] - Total Samples Processed: 1000\n",
            "Epoch [125/500] - Total Samples Processed: 1000\n",
            "Epoch [126/500] - Total Samples Processed: 1000\n",
            "Epoch [127/500] - Total Samples Processed: 1000\n",
            "Epoch [128/500] - Total Samples Processed: 1000\n",
            "Epoch [129/500] - Total Samples Processed: 1000\n",
            "Epoch [130/500] - Total Samples Processed: 1000\n",
            "Epoch [131/500] - Total Samples Processed: 1000\n",
            "Epoch [132/500] - Total Samples Processed: 1000\n",
            "Epoch [133/500] - Total Samples Processed: 1000\n",
            "Epoch [134/500] - Total Samples Processed: 1000\n",
            "Epoch [135/500] - Total Samples Processed: 1000\n",
            "Epoch [136/500] - Total Samples Processed: 1000\n",
            "Epoch [137/500] - Total Samples Processed: 1000\n",
            "Epoch [138/500] - Total Samples Processed: 1000\n",
            "Epoch [139/500] - Total Samples Processed: 1000\n",
            "Epoch [140/500] - Total Samples Processed: 1000\n",
            "Epoch [141/500] - Total Samples Processed: 1000\n",
            "Epoch [142/500] - Total Samples Processed: 1000\n",
            "Epoch [143/500] - Total Samples Processed: 1000\n",
            "Epoch [144/500] - Total Samples Processed: 1000\n",
            "Epoch [145/500] - Total Samples Processed: 1000\n",
            "Epoch [146/500] - Total Samples Processed: 1000\n",
            "Epoch [147/500] - Total Samples Processed: 1000\n",
            "Epoch [148/500] - Total Samples Processed: 1000\n",
            "Epoch [149/500] - Total Samples Processed: 1000\n",
            "Epoch [150/500] - Total Samples Processed: 1000\n",
            "Epoch [151/500] - Total Samples Processed: 1000\n",
            "Epoch [152/500] - Total Samples Processed: 1000\n",
            "Epoch [153/500] - Total Samples Processed: 1000\n",
            "Epoch [154/500] - Total Samples Processed: 1000\n",
            "Epoch [155/500] - Total Samples Processed: 1000\n",
            "Epoch [156/500] - Total Samples Processed: 1000\n",
            "Epoch [157/500] - Total Samples Processed: 1000\n",
            "Epoch [158/500] - Total Samples Processed: 1000\n",
            "Epoch [159/500] - Total Samples Processed: 1000\n",
            "Epoch [160/500] - Total Samples Processed: 1000\n",
            "Epoch [161/500] - Total Samples Processed: 1000\n",
            "Epoch [162/500] - Total Samples Processed: 1000\n",
            "Epoch [163/500] - Total Samples Processed: 1000\n",
            "Epoch [164/500] - Total Samples Processed: 1000\n",
            "Epoch [165/500] - Total Samples Processed: 1000\n",
            "Epoch [166/500] - Total Samples Processed: 1000\n",
            "Epoch [167/500] - Total Samples Processed: 1000\n",
            "Epoch [168/500] - Total Samples Processed: 1000\n",
            "Epoch [169/500] - Total Samples Processed: 1000\n",
            "Epoch [170/500] - Total Samples Processed: 1000\n",
            "Epoch [171/500] - Total Samples Processed: 1000\n",
            "Epoch [172/500] - Total Samples Processed: 1000\n",
            "Epoch [173/500] - Total Samples Processed: 1000\n",
            "Epoch [174/500] - Total Samples Processed: 1000\n",
            "Epoch [175/500] - Total Samples Processed: 1000\n",
            "Epoch [176/500] - Total Samples Processed: 1000\n",
            "Epoch [177/500] - Total Samples Processed: 1000\n",
            "Epoch [178/500] - Total Samples Processed: 1000\n",
            "Epoch [179/500] - Total Samples Processed: 1000\n",
            "Epoch [180/500] - Total Samples Processed: 1000\n",
            "Epoch [181/500] - Total Samples Processed: 1000\n",
            "Epoch [182/500] - Total Samples Processed: 1000\n",
            "Epoch [183/500] - Total Samples Processed: 1000\n",
            "Epoch [184/500] - Total Samples Processed: 1000\n",
            "Epoch [185/500] - Total Samples Processed: 1000\n",
            "Epoch [186/500] - Total Samples Processed: 1000\n",
            "Epoch [187/500] - Total Samples Processed: 1000\n",
            "Epoch [188/500] - Total Samples Processed: 1000\n",
            "Epoch [189/500] - Total Samples Processed: 1000\n",
            "Epoch [190/500] - Total Samples Processed: 1000\n",
            "Epoch [191/500] - Total Samples Processed: 1000\n",
            "Epoch [192/500] - Total Samples Processed: 1000\n",
            "Epoch [193/500] - Total Samples Processed: 1000\n",
            "Epoch [194/500] - Total Samples Processed: 1000\n",
            "Epoch [195/500] - Total Samples Processed: 1000\n",
            "Epoch [196/500] - Total Samples Processed: 1000\n",
            "Epoch [197/500] - Total Samples Processed: 1000\n",
            "Epoch [198/500] - Total Samples Processed: 1000\n",
            "Epoch [199/500] - Total Samples Processed: 1000\n",
            "Epoch [200/500] - Total Samples Processed: 1000\n",
            "Epoch [201/500] - Total Samples Processed: 1000\n",
            "Epoch [202/500] - Total Samples Processed: 1000\n",
            "Epoch [203/500] - Total Samples Processed: 1000\n",
            "Epoch [204/500] - Total Samples Processed: 1000\n",
            "Epoch [205/500] - Total Samples Processed: 1000\n",
            "Epoch [206/500] - Total Samples Processed: 1000\n",
            "Epoch [207/500] - Total Samples Processed: 1000\n",
            "Epoch [208/500] - Total Samples Processed: 1000\n",
            "Epoch [209/500] - Total Samples Processed: 1000\n",
            "Epoch [210/500] - Total Samples Processed: 1000\n",
            "Epoch [211/500] - Total Samples Processed: 1000\n",
            "Epoch [212/500] - Total Samples Processed: 1000\n",
            "Epoch [213/500] - Total Samples Processed: 1000\n",
            "Epoch [214/500] - Total Samples Processed: 1000\n",
            "Epoch [215/500] - Total Samples Processed: 1000\n",
            "Epoch [216/500] - Total Samples Processed: 1000\n",
            "Epoch [217/500] - Total Samples Processed: 1000\n",
            "Epoch [218/500] - Total Samples Processed: 1000\n",
            "Epoch [219/500] - Total Samples Processed: 1000\n",
            "Epoch [220/500] - Total Samples Processed: 1000\n",
            "Epoch [221/500] - Total Samples Processed: 1000\n",
            "Epoch [222/500] - Total Samples Processed: 1000\n",
            "Epoch [223/500] - Total Samples Processed: 1000\n",
            "Epoch [224/500] - Total Samples Processed: 1000\n",
            "Epoch [225/500] - Total Samples Processed: 1000\n",
            "Epoch [226/500] - Total Samples Processed: 1000\n",
            "Epoch [227/500] - Total Samples Processed: 1000\n",
            "Epoch [228/500] - Total Samples Processed: 1000\n",
            "Epoch [229/500] - Total Samples Processed: 1000\n",
            "Epoch [230/500] - Total Samples Processed: 1000\n",
            "Epoch [231/500] - Total Samples Processed: 1000\n",
            "Epoch [232/500] - Total Samples Processed: 1000\n",
            "Epoch [233/500] - Total Samples Processed: 1000\n",
            "Epoch [234/500] - Total Samples Processed: 1000\n",
            "Epoch [235/500] - Total Samples Processed: 1000\n",
            "Epoch [236/500] - Total Samples Processed: 1000\n",
            "Epoch [237/500] - Total Samples Processed: 1000\n",
            "Epoch [238/500] - Total Samples Processed: 1000\n",
            "Epoch [239/500] - Total Samples Processed: 1000\n",
            "Epoch [240/500] - Total Samples Processed: 1000\n",
            "Epoch [241/500] - Total Samples Processed: 1000\n",
            "Epoch [242/500] - Total Samples Processed: 1000\n",
            "Epoch [243/500] - Total Samples Processed: 1000\n",
            "Epoch [244/500] - Total Samples Processed: 1000\n",
            "Epoch [245/500] - Total Samples Processed: 1000\n",
            "Epoch [246/500] - Total Samples Processed: 1000\n",
            "Epoch [247/500] - Total Samples Processed: 1000\n",
            "Epoch [248/500] - Total Samples Processed: 1000\n",
            "Epoch [249/500] - Total Samples Processed: 1000\n",
            "Epoch [250/500] - Total Samples Processed: 1000\n",
            "Epoch [251/500] - Total Samples Processed: 1000\n",
            "Epoch [252/500] - Total Samples Processed: 1000\n",
            "Epoch [253/500] - Total Samples Processed: 1000\n",
            "Epoch [254/500] - Total Samples Processed: 1000\n",
            "Epoch [255/500] - Total Samples Processed: 1000\n",
            "Epoch [256/500] - Total Samples Processed: 1000\n",
            "Epoch [257/500] - Total Samples Processed: 1000\n",
            "Epoch [258/500] - Total Samples Processed: 1000\n",
            "Epoch [259/500] - Total Samples Processed: 1000\n",
            "Epoch [260/500] - Total Samples Processed: 1000\n",
            "Epoch [261/500] - Total Samples Processed: 1000\n",
            "Epoch [262/500] - Total Samples Processed: 1000\n",
            "Epoch [263/500] - Total Samples Processed: 1000\n",
            "Epoch [264/500] - Total Samples Processed: 1000\n",
            "Epoch [265/500] - Total Samples Processed: 1000\n",
            "Epoch [266/500] - Total Samples Processed: 1000\n",
            "Epoch [267/500] - Total Samples Processed: 1000\n",
            "Epoch [268/500] - Total Samples Processed: 1000\n",
            "Epoch [269/500] - Total Samples Processed: 1000\n",
            "Epoch [270/500] - Total Samples Processed: 1000\n",
            "Epoch [271/500] - Total Samples Processed: 1000\n",
            "Epoch [272/500] - Total Samples Processed: 1000\n",
            "Epoch [273/500] - Total Samples Processed: 1000\n",
            "Epoch [274/500] - Total Samples Processed: 1000\n",
            "Epoch [275/500] - Total Samples Processed: 1000\n",
            "Epoch [276/500] - Total Samples Processed: 1000\n",
            "Epoch [277/500] - Total Samples Processed: 1000\n",
            "Epoch [278/500] - Total Samples Processed: 1000\n",
            "Epoch [279/500] - Total Samples Processed: 1000\n",
            "Epoch [280/500] - Total Samples Processed: 1000\n",
            "Epoch [281/500] - Total Samples Processed: 1000\n",
            "Epoch [282/500] - Total Samples Processed: 1000\n",
            "Epoch [283/500] - Total Samples Processed: 1000\n",
            "Epoch [284/500] - Total Samples Processed: 1000\n",
            "Epoch [285/500] - Total Samples Processed: 1000\n",
            "Epoch [286/500] - Total Samples Processed: 1000\n",
            "Epoch [287/500] - Total Samples Processed: 1000\n",
            "Epoch [288/500] - Total Samples Processed: 1000\n",
            "Epoch [289/500] - Total Samples Processed: 1000\n",
            "Epoch [290/500] - Total Samples Processed: 1000\n",
            "Epoch [291/500] - Total Samples Processed: 1000\n",
            "Epoch [292/500] - Total Samples Processed: 1000\n",
            "Epoch [293/500] - Total Samples Processed: 1000\n",
            "Epoch [294/500] - Total Samples Processed: 1000\n",
            "Epoch [295/500] - Total Samples Processed: 1000\n",
            "Epoch [296/500] - Total Samples Processed: 1000\n",
            "Epoch [297/500] - Total Samples Processed: 1000\n",
            "Epoch [298/500] - Total Samples Processed: 1000\n",
            "Epoch [299/500] - Total Samples Processed: 1000\n",
            "Epoch [300/500] - Total Samples Processed: 1000\n",
            "Epoch [301/500] - Total Samples Processed: 1000\n",
            "Epoch [302/500] - Total Samples Processed: 1000\n",
            "Epoch [303/500] - Total Samples Processed: 1000\n",
            "Epoch [304/500] - Total Samples Processed: 1000\n",
            "Epoch [305/500] - Total Samples Processed: 1000\n",
            "Epoch [306/500] - Total Samples Processed: 1000\n",
            "Epoch [307/500] - Total Samples Processed: 1000\n",
            "Epoch [308/500] - Total Samples Processed: 1000\n",
            "Epoch [309/500] - Total Samples Processed: 1000\n",
            "Epoch [310/500] - Total Samples Processed: 1000\n",
            "Epoch [311/500] - Total Samples Processed: 1000\n",
            "Epoch [312/500] - Total Samples Processed: 1000\n",
            "Epoch [313/500] - Total Samples Processed: 1000\n",
            "Epoch [314/500] - Total Samples Processed: 1000\n",
            "Epoch [315/500] - Total Samples Processed: 1000\n",
            "Epoch [316/500] - Total Samples Processed: 1000\n",
            "Epoch [317/500] - Total Samples Processed: 1000\n",
            "Epoch [318/500] - Total Samples Processed: 1000\n",
            "Epoch [319/500] - Total Samples Processed: 1000\n",
            "Epoch [320/500] - Total Samples Processed: 1000\n",
            "Epoch [321/500] - Total Samples Processed: 1000\n",
            "Epoch [322/500] - Total Samples Processed: 1000\n",
            "Epoch [323/500] - Total Samples Processed: 1000\n",
            "Epoch [324/500] - Total Samples Processed: 1000\n",
            "Epoch [325/500] - Total Samples Processed: 1000\n",
            "Epoch [326/500] - Total Samples Processed: 1000\n",
            "Epoch [327/500] - Total Samples Processed: 1000\n",
            "Epoch [328/500] - Total Samples Processed: 1000\n",
            "Epoch [329/500] - Total Samples Processed: 1000\n",
            "Epoch [330/500] - Total Samples Processed: 1000\n",
            "Epoch [331/500] - Total Samples Processed: 1000\n",
            "Epoch [332/500] - Total Samples Processed: 1000\n",
            "Epoch [333/500] - Total Samples Processed: 1000\n",
            "Epoch [334/500] - Total Samples Processed: 1000\n",
            "Epoch [335/500] - Total Samples Processed: 1000\n",
            "Epoch [336/500] - Total Samples Processed: 1000\n",
            "Epoch [337/500] - Total Samples Processed: 1000\n",
            "Epoch [338/500] - Total Samples Processed: 1000\n",
            "Epoch [339/500] - Total Samples Processed: 1000\n",
            "Epoch [340/500] - Total Samples Processed: 1000\n",
            "Epoch [341/500] - Total Samples Processed: 1000\n",
            "Epoch [342/500] - Total Samples Processed: 1000\n",
            "Epoch [343/500] - Total Samples Processed: 1000\n",
            "Epoch [344/500] - Total Samples Processed: 1000\n",
            "Epoch [345/500] - Total Samples Processed: 1000\n",
            "Epoch [346/500] - Total Samples Processed: 1000\n",
            "Epoch [347/500] - Total Samples Processed: 1000\n",
            "Epoch [348/500] - Total Samples Processed: 1000\n",
            "Epoch [349/500] - Total Samples Processed: 1000\n",
            "Epoch [350/500] - Total Samples Processed: 1000\n",
            "Epoch [351/500] - Total Samples Processed: 1000\n",
            "Epoch [352/500] - Total Samples Processed: 1000\n",
            "Epoch [353/500] - Total Samples Processed: 1000\n",
            "Epoch [354/500] - Total Samples Processed: 1000\n",
            "Epoch [355/500] - Total Samples Processed: 1000\n",
            "Epoch [356/500] - Total Samples Processed: 1000\n",
            "Epoch [357/500] - Total Samples Processed: 1000\n",
            "Epoch [358/500] - Total Samples Processed: 1000\n",
            "Epoch [359/500] - Total Samples Processed: 1000\n",
            "Epoch [360/500] - Total Samples Processed: 1000\n",
            "Epoch [361/500] - Total Samples Processed: 1000\n",
            "Epoch [362/500] - Total Samples Processed: 1000\n",
            "Epoch [363/500] - Total Samples Processed: 1000\n",
            "Epoch [364/500] - Total Samples Processed: 1000\n",
            "Epoch [365/500] - Total Samples Processed: 1000\n",
            "Epoch [366/500] - Total Samples Processed: 1000\n",
            "Epoch [367/500] - Total Samples Processed: 1000\n",
            "Epoch [368/500] - Total Samples Processed: 1000\n",
            "Epoch [369/500] - Total Samples Processed: 1000\n",
            "Epoch [370/500] - Total Samples Processed: 1000\n",
            "Epoch [371/500] - Total Samples Processed: 1000\n",
            "Epoch [372/500] - Total Samples Processed: 1000\n",
            "Epoch [373/500] - Total Samples Processed: 1000\n",
            "Epoch [374/500] - Total Samples Processed: 1000\n",
            "Epoch [375/500] - Total Samples Processed: 1000\n",
            "Epoch [376/500] - Total Samples Processed: 1000\n",
            "Epoch [377/500] - Total Samples Processed: 1000\n",
            "Epoch [378/500] - Total Samples Processed: 1000\n",
            "Epoch [379/500] - Total Samples Processed: 1000\n",
            "Epoch [380/500] - Total Samples Processed: 1000\n",
            "Epoch [381/500] - Total Samples Processed: 1000\n",
            "Epoch [382/500] - Total Samples Processed: 1000\n",
            "Epoch [383/500] - Total Samples Processed: 1000\n",
            "Epoch [384/500] - Total Samples Processed: 1000\n",
            "Epoch [385/500] - Total Samples Processed: 1000\n",
            "Epoch [386/500] - Total Samples Processed: 1000\n",
            "Epoch [387/500] - Total Samples Processed: 1000\n",
            "Epoch [388/500] - Total Samples Processed: 1000\n",
            "Epoch [389/500] - Total Samples Processed: 1000\n",
            "Epoch [390/500] - Total Samples Processed: 1000\n",
            "Epoch [391/500] - Total Samples Processed: 1000\n",
            "Epoch [392/500] - Total Samples Processed: 1000\n",
            "Epoch [393/500] - Total Samples Processed: 1000\n",
            "Epoch [394/500] - Total Samples Processed: 1000\n",
            "Epoch [395/500] - Total Samples Processed: 1000\n",
            "Epoch [396/500] - Total Samples Processed: 1000\n",
            "Epoch [397/500] - Total Samples Processed: 1000\n",
            "Epoch [398/500] - Total Samples Processed: 1000\n",
            "Epoch [399/500] - Total Samples Processed: 1000\n",
            "Epoch [400/500] - Total Samples Processed: 1000\n",
            "Epoch [401/500] - Total Samples Processed: 1000\n",
            "Epoch [402/500] - Total Samples Processed: 1000\n",
            "Epoch [403/500] - Total Samples Processed: 1000\n",
            "Epoch [404/500] - Total Samples Processed: 1000\n",
            "Epoch [405/500] - Total Samples Processed: 1000\n",
            "Epoch [406/500] - Total Samples Processed: 1000\n",
            "Epoch [407/500] - Total Samples Processed: 1000\n",
            "Epoch [408/500] - Total Samples Processed: 1000\n",
            "Epoch [409/500] - Total Samples Processed: 1000\n",
            "Epoch [410/500] - Total Samples Processed: 1000\n",
            "Epoch [411/500] - Total Samples Processed: 1000\n",
            "Epoch [412/500] - Total Samples Processed: 1000\n",
            "Epoch [413/500] - Total Samples Processed: 1000\n",
            "Epoch [414/500] - Total Samples Processed: 1000\n",
            "Epoch [415/500] - Total Samples Processed: 1000\n",
            "Epoch [416/500] - Total Samples Processed: 1000\n",
            "Epoch [417/500] - Total Samples Processed: 1000\n",
            "Epoch [418/500] - Total Samples Processed: 1000\n",
            "Epoch [419/500] - Total Samples Processed: 1000\n",
            "Epoch [420/500] - Total Samples Processed: 1000\n",
            "Epoch [421/500] - Total Samples Processed: 1000\n",
            "Epoch [422/500] - Total Samples Processed: 1000\n",
            "Epoch [423/500] - Total Samples Processed: 1000\n",
            "Epoch [424/500] - Total Samples Processed: 1000\n",
            "Epoch [425/500] - Total Samples Processed: 1000\n",
            "Epoch [426/500] - Total Samples Processed: 1000\n",
            "Epoch [427/500] - Total Samples Processed: 1000\n",
            "Epoch [428/500] - Total Samples Processed: 1000\n",
            "Epoch [429/500] - Total Samples Processed: 1000\n",
            "Epoch [430/500] - Total Samples Processed: 1000\n",
            "Epoch [431/500] - Total Samples Processed: 1000\n",
            "Epoch [432/500] - Total Samples Processed: 1000\n",
            "Epoch [433/500] - Total Samples Processed: 1000\n",
            "Epoch [434/500] - Total Samples Processed: 1000\n",
            "Epoch [435/500] - Total Samples Processed: 1000\n",
            "Epoch [436/500] - Total Samples Processed: 1000\n",
            "Epoch [437/500] - Total Samples Processed: 1000\n",
            "Epoch [438/500] - Total Samples Processed: 1000\n",
            "Epoch [439/500] - Total Samples Processed: 1000\n",
            "Epoch [440/500] - Total Samples Processed: 1000\n",
            "Epoch [441/500] - Total Samples Processed: 1000\n",
            "Epoch [442/500] - Total Samples Processed: 1000\n",
            "Epoch [443/500] - Total Samples Processed: 1000\n",
            "Epoch [444/500] - Total Samples Processed: 1000\n",
            "Epoch [445/500] - Total Samples Processed: 1000\n",
            "Epoch [446/500] - Total Samples Processed: 1000\n",
            "Epoch [447/500] - Total Samples Processed: 1000\n",
            "Epoch [448/500] - Total Samples Processed: 1000\n",
            "Epoch [449/500] - Total Samples Processed: 1000\n",
            "Epoch [450/500] - Total Samples Processed: 1000\n",
            "Epoch [451/500] - Total Samples Processed: 1000\n",
            "Epoch [452/500] - Total Samples Processed: 1000\n",
            "Epoch [453/500] - Total Samples Processed: 1000\n",
            "Epoch [454/500] - Total Samples Processed: 1000\n",
            "Epoch [455/500] - Total Samples Processed: 1000\n",
            "Epoch [456/500] - Total Samples Processed: 1000\n",
            "Epoch [457/500] - Total Samples Processed: 1000\n",
            "Epoch [458/500] - Total Samples Processed: 1000\n",
            "Epoch [459/500] - Total Samples Processed: 1000\n",
            "Epoch [460/500] - Total Samples Processed: 1000\n",
            "Epoch [461/500] - Total Samples Processed: 1000\n",
            "Epoch [462/500] - Total Samples Processed: 1000\n",
            "Epoch [463/500] - Total Samples Processed: 1000\n",
            "Epoch [464/500] - Total Samples Processed: 1000\n",
            "Epoch [465/500] - Total Samples Processed: 1000\n",
            "Epoch [466/500] - Total Samples Processed: 1000\n",
            "Epoch [467/500] - Total Samples Processed: 1000\n",
            "Epoch [468/500] - Total Samples Processed: 1000\n",
            "Epoch [469/500] - Total Samples Processed: 1000\n",
            "Epoch [470/500] - Total Samples Processed: 1000\n",
            "Epoch [471/500] - Total Samples Processed: 1000\n",
            "Epoch [472/500] - Total Samples Processed: 1000\n",
            "Epoch [473/500] - Total Samples Processed: 1000\n",
            "Epoch [474/500] - Total Samples Processed: 1000\n",
            "Epoch [475/500] - Total Samples Processed: 1000\n",
            "Epoch [476/500] - Total Samples Processed: 1000\n",
            "Epoch [477/500] - Total Samples Processed: 1000\n",
            "Epoch [478/500] - Total Samples Processed: 1000\n",
            "Epoch [479/500] - Total Samples Processed: 1000\n",
            "Epoch [480/500] - Total Samples Processed: 1000\n",
            "Epoch [481/500] - Total Samples Processed: 1000\n",
            "Epoch [482/500] - Total Samples Processed: 1000\n",
            "Epoch [483/500] - Total Samples Processed: 1000\n",
            "Epoch [484/500] - Total Samples Processed: 1000\n",
            "Epoch [485/500] - Total Samples Processed: 1000\n",
            "Epoch [486/500] - Total Samples Processed: 1000\n",
            "Epoch [487/500] - Total Samples Processed: 1000\n",
            "Epoch [488/500] - Total Samples Processed: 1000\n",
            "Epoch [489/500] - Total Samples Processed: 1000\n",
            "Epoch [490/500] - Total Samples Processed: 1000\n",
            "Epoch [491/500] - Total Samples Processed: 1000\n",
            "Epoch [492/500] - Total Samples Processed: 1000\n",
            "Epoch [493/500] - Total Samples Processed: 1000\n",
            "Epoch [494/500] - Total Samples Processed: 1000\n",
            "Epoch [495/500] - Total Samples Processed: 1000\n",
            "Epoch [496/500] - Total Samples Processed: 1000\n",
            "Epoch [497/500] - Total Samples Processed: 1000\n",
            "Epoch [498/500] - Total Samples Processed: 1000\n",
            "Epoch [499/500] - Total Samples Processed: 1000\n",
            "Epoch [500/500] - Total Samples Processed: 1000\n",
            "GAN trained successfully!\n",
            "Using cpu device\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 978  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 2    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 704         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008900609 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.686      |\n",
            "|    explained_variance   | -0.000295   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.75        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    value_loss           | 52.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 659        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 9          |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01140829 |\n",
            "|    clip_fraction        | 0.0835     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.663     |\n",
            "|    explained_variance   | 0.0721     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 11.7       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0218    |\n",
            "|    value_loss           | 38.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 652         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008841533 |\n",
            "|    clip_fraction        | 0.076       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.637      |\n",
            "|    explained_variance   | 0.228       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 30.2        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    value_loss           | 60          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 650         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007832447 |\n",
            "|    clip_fraction        | 0.0647      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.609      |\n",
            "|    explained_variance   | 0.274       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 23.6        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0188     |\n",
            "|    value_loss           | 67.4        |\n",
            "-----------------------------------------\n",
            "RL Agent trained successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import foolbox as fb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load CICIDS2017 dataset\n",
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.fillna(0, inplace=True)  # Handle missing values\n",
        "    return df[:1000]  # Use only first 5000 data points\n",
        "\n",
        "# Normalize dataset\n",
        "def normalize_data(df):\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df = df[numerical_cols]  # Drop non-numeric columns\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "    df = df.astype(np.float32)\n",
        "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "    return df\n",
        "\n",
        "# Define GAN for adversarial attack generation\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Train GAN\n",
        "def train_gan(generator, discriminator, data, epochs=500, batch_size=128, lr=0.0002):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer_g = optim.AdamW(generator.parameters(), lr=lr)\n",
        "    optimizer_d = optim.AdamW(discriminator.parameters(), lr=lr)\n",
        "    dataloader = DataLoader(TensorDataset(data), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_samples = 0\n",
        "        for real_samples in dataloader:\n",
        "            real_samples = real_samples[0]\n",
        "            batch_size = real_samples.size(0)\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # Generate fake samples\n",
        "            noise = torch.randn(batch_size, real_samples.shape[1])\n",
        "            fake_samples = generator(noise)\n",
        "\n",
        "            # Train Discriminator\n",
        "            real_labels = torch.ones(batch_size, 1)\n",
        "            fake_labels = torch.zeros(batch_size, 1)\n",
        "            optimizer_d.zero_grad()\n",
        "            loss_real = criterion(discriminator(real_samples), real_labels)\n",
        "            loss_fake = criterion(discriminator(fake_samples.detach()), fake_labels)\n",
        "            loss_d = loss_real + loss_fake\n",
        "            loss_d.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Train Generator\n",
        "            optimizer_g.zero_grad()\n",
        "            loss_g = criterion(discriminator(fake_samples), real_labels)\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Total Samples Processed: {total_samples}\")\n",
        "\n",
        "    return generator\n",
        "\n",
        "# Example usage\n",
        "dataset_path = \"/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"  # Replace with actual path\n",
        "df = load_dataset(dataset_path)\n",
        "df = normalize_data(df)\n",
        "print(df.dtypes)\n",
        "print(df.head())\n",
        "print(\"Generator Start Running...\")\n",
        "generator = Generator(input_dim=df.shape[1], output_dim=df.shape[1])\n",
        "print(\"Generator Stop Running...\")\n",
        "\n",
        "print(\"Discriminator Start Running...\")\n",
        "discriminator = Discriminator(input_dim=df.shape[1])\n",
        "print(\"Discriminator Stop Running...\")\n",
        "\n",
        "trained_generator = train_gan(generator, discriminator, torch.tensor(df.values, dtype=torch.float32))\n",
        "print(\"GAN trained successfully!\")\n",
        "\n",
        "import gymnasium as gym  # Use Gymnasium instead of Gym\n",
        "import stable_baselines3 as sb3\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Reinforcement Learning for Adaptive Defense\n",
        "def train_rl_agent(env_name=\"CartPole-v1\", timesteps=10000):\n",
        "    # Create environment and wrap it with DummyVecEnv\n",
        "    env = gym.make(env_name)  # Gymnasium environment\n",
        "    env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv\n",
        "\n",
        "    # Reset the environment and handle dictionary structure for gymnasium\n",
        "    reset_output = env.reset()  # Reset and get the output (this will be a dictionary)\n",
        "    obs = reset_output[0]  # First element is the observation (from DummyVecEnv)\n",
        "    info = reset_output[1] if isinstance(reset_output, tuple) else {}  # Extract info if available\n",
        "\n",
        "    model = sb3.PPO(\"MlpPolicy\", env, verbose=1)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    return model\n",
        "\n",
        "# Train Reinforcement Learning Agent\n",
        "rl_model = train_rl_agent()\n",
        "print(\"RL Agent trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "gN2UsYreMu_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install foolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw9updQnodWE",
        "outputId": "4ad809f0-8e2e-4897-b316-f76b5d434844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting foolbox\n",
            "  Downloading foolbox-3.3.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from foolbox) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from foolbox) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from foolbox) (75.1.0)\n",
            "Collecting eagerpy>=0.30.0 (from foolbox)\n",
            "  Downloading eagerpy-0.30.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: GitPython>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from foolbox) (3.1.44)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.11/dist-packages (from foolbox) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from foolbox) (2.32.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.0.7->foolbox) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->foolbox) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->foolbox) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->foolbox) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->foolbox) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.0.7->foolbox) (5.0.2)\n",
            "Downloading foolbox-3.3.4-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eagerpy-0.30.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: eagerpy, foolbox\n",
            "Successfully installed eagerpy-0.30.0 foolbox-3.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym  # Use Gymnasium instead of Gym\n",
        "import stable_baselines3 as sb3\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "import flwr as fl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# After normalizing the dataset (after 'normalize_data(df)' call):\n",
        "def create_graph_from_data(df):\n",
        "    num_nodes = len(df)\n",
        "    edges = np.random.randint(0, num_nodes, size=(2, 100))  # Adjust as per real connection logic\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "\n",
        "    features = torch.tensor(df.values, dtype=torch.float)\n",
        "\n",
        "    data = Data(x=features, edge_index=edge_index)\n",
        "    return data\n",
        "\n",
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_channels * 8, out_channels, heads=1, dropout=0.6)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.nn.functional.elu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def train_gat_model(data, epochs=100, lr=0.001):\n",
        "    model = GATModel(in_channels=data.num_node_features, hidden_channels=64, out_channels=1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.x, data.edge_index)  # Forward pass\n",
        "        loss = criterion(output, torch.zeros_like(output))  # Adjust target for anomaly detection\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item()}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data):\n",
        "    model.eval()\n",
        "    output = model(data.x, data.edge_index)\n",
        "    anomalous_nodes = output > 0.5  # Anomaly detection threshold\n",
        "    print(f\"Anomalous Nodes: {anomalous_nodes.nonzero()}\")\n",
        "\n",
        "class FLModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FLModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)  # Binary classification (for simplicity)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "class Client(fl.client.NumPyClient):\n",
        "    def __init__(self, model, data, targets):\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return [val.cpu().numpy() for val in self.model.parameters()]\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params = zip(self.model.parameters(), parameters)\n",
        "        for param, value in params:\n",
        "            param.data = torch.tensor(value, dtype=torch.float32)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(self.data)\n",
        "        loss = self.criterion(output.squeeze(), self.targets.float())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return self.get_parameters(), len(self.data), loss.item()\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        with torch.no_grad():\n",
        "            output = self.model(self.data)\n",
        "            loss = self.criterion(output.squeeze(), self.targets.float())\n",
        "        return loss.item(), len(self.data), {\"accuracy\": (output.round() == self.targets).float().mean().item()}\n",
        "\n",
        "def split_dataset(df, num_clients=5):\n",
        "    # Split the dataframe into smaller chunks (simulate multiple nodes)\n",
        "    chunk_size = len(df) // num_clients\n",
        "    clients_data = []\n",
        "    clients_targets = []\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        chunk = df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
        "        data = torch.tensor(chunk.values, dtype=torch.float32)\n",
        "        targets = torch.tensor(np.random.randint(0, 2, size=(len(chunk),)), dtype=torch.long)  # Random binary target\n",
        "        clients_data.append(data)\n",
        "        clients_targets.append(targets)\n",
        "\n",
        "    return clients_data, clients_targets\n",
        "\n",
        "from flwr.server.strategy import FedAvg\n",
        "from flwr.server import start_server\n",
        "from flwr.server.server import ServerConfig\n",
        "\n",
        "def start_federated_learning():\n",
        "    # Initialize the global model\n",
        "    # global_model = FLModel(input_dim=df.shape[1])\n",
        "\n",
        "    # # Split dataset for clients (representing power grid nodes)\n",
        "    # clients_data, clients_targets = split_dataset(df, num_clients=5)\n",
        "\n",
        "    # # Create clients\n",
        "    # clients = []\n",
        "    # for i in range(len(clients_data)):\n",
        "    #     client = Client(global_model, clients_data[i], clients_targets[i])\n",
        "    #     clients.append(client)\n",
        "\n",
        "    # # Define the federated learning strategy\n",
        "    # strategy = fl.server.strategy.FedAvg(\n",
        "    #     fraction_fit=0.5,  # Fraction of clients to use for training\n",
        "    #     min_available_clients=2  # Ensure at least 2 clients are available for training\n",
        "    # )\n",
        "\n",
        "    # Start the Flower server for federated learning\n",
        "    strategy = FedAvg(\n",
        "      fraction_fit=0.5,  # Fraction of clients used for training\n",
        "      min_fit_clients=2,  # Minimum number of clients needed for training\n",
        "      min_available_clients=2  # Ensure at least 2 clients are available\n",
        "    )\n",
        "\n",
        "     # Create ServerConfig object\n",
        "    config = ServerConfig(num_rounds=1)\n",
        "\n",
        "    # Start the FL server using ServerConfig\n",
        "    start_server(server_address=\"0.0.0.0:8085\", config=config)\n",
        "\n",
        "\n",
        "# Reinforcement Learning for Adaptive Defense\n",
        "def train_rl_agent(env_name=\"CartPole-v1\", timesteps=10000):\n",
        "    # Create environment and wrap it with DummyVecEnv\n",
        "    env = gym.make(env_name)  # Gymnasium environment\n",
        "    env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv\n",
        "\n",
        "    # Reset the environment and handle dictionary structure for gymnasium\n",
        "    reset_output = env.reset()  # Reset and get the output (this will be a dictionary)\n",
        "    obs = reset_output[0]  # First element is the observation (from DummyVecEnv)\n",
        "    info = reset_output[1] if isinstance(reset_output, tuple) else {}  # Extract info if available\n",
        "\n",
        "    model = sb3.PPO(\"MlpPolicy\", env, verbose=1)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    return model\n",
        "\n",
        "# Step 3: Create Graph from Data (Graph Representation)\n",
        "data = create_graph_from_data(df)\n",
        "\n",
        "# Step 4: Train GAT Model for Anomaly Detection\n",
        "gat_model = train_gat_model(data)\n",
        "\n",
        "# Step 5: Evaluate GAT Model for Anomalies\n",
        "evaluate_model(gat_model, data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtFc7T2dvEbb",
        "outputId": "03d5ec91-3543-41d5-99d0-005b7f6f52f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] - Loss: 0.6654284596443176\n",
            "Epoch [2/100] - Loss: 0.6031944751739502\n",
            "Epoch [3/100] - Loss: 0.5626355409622192\n",
            "Epoch [4/100] - Loss: 0.5301454067230225\n",
            "Epoch [5/100] - Loss: 0.5158560872077942\n",
            "Epoch [6/100] - Loss: 0.49301624298095703\n",
            "Epoch [7/100] - Loss: 0.4675811231136322\n",
            "Epoch [8/100] - Loss: 0.46819007396698\n",
            "Epoch [9/100] - Loss: 0.4452321529388428\n",
            "Epoch [10/100] - Loss: 0.4394039213657379\n",
            "Epoch [11/100] - Loss: 0.448520302772522\n",
            "Epoch [12/100] - Loss: 0.43978312611579895\n",
            "Epoch [13/100] - Loss: 0.4143456816673279\n",
            "Epoch [14/100] - Loss: 0.438574880361557\n",
            "Epoch [15/100] - Loss: 0.42987436056137085\n",
            "Epoch [16/100] - Loss: 0.403044730424881\n",
            "Epoch [17/100] - Loss: 0.41572850942611694\n",
            "Epoch [18/100] - Loss: 0.4182533621788025\n",
            "Epoch [19/100] - Loss: 0.40243732929229736\n",
            "Epoch [20/100] - Loss: 0.41281768679618835\n",
            "Epoch [21/100] - Loss: 0.3998563885688782\n",
            "Epoch [22/100] - Loss: 0.4058057963848114\n",
            "Epoch [23/100] - Loss: 0.3900117874145508\n",
            "Epoch [24/100] - Loss: 0.39360061287879944\n",
            "Epoch [25/100] - Loss: 0.3953397870063782\n",
            "Epoch [26/100] - Loss: 0.4074343740940094\n",
            "Epoch [27/100] - Loss: 0.3909529149532318\n",
            "Epoch [28/100] - Loss: 0.3954201638698578\n",
            "Epoch [29/100] - Loss: 0.4057892858982086\n",
            "Epoch [30/100] - Loss: 0.40849769115448\n",
            "Epoch [31/100] - Loss: 0.3959813416004181\n",
            "Epoch [32/100] - Loss: 0.3847276568412781\n",
            "Epoch [33/100] - Loss: 0.38392260670661926\n",
            "Epoch [34/100] - Loss: 0.40955886244773865\n",
            "Epoch [35/100] - Loss: 0.3833414316177368\n",
            "Epoch [36/100] - Loss: 0.38999927043914795\n",
            "Epoch [37/100] - Loss: 0.3879746198654175\n",
            "Epoch [38/100] - Loss: 0.3984270691871643\n",
            "Epoch [39/100] - Loss: 0.3989064693450928\n",
            "Epoch [40/100] - Loss: 0.38859444856643677\n",
            "Epoch [41/100] - Loss: 0.4116871953010559\n",
            "Epoch [42/100] - Loss: 0.39747872948646545\n",
            "Epoch [43/100] - Loss: 0.3858710825443268\n",
            "Epoch [44/100] - Loss: 0.3936028480529785\n",
            "Epoch [45/100] - Loss: 0.4051162600517273\n",
            "Epoch [46/100] - Loss: 0.4007780849933624\n",
            "Epoch [47/100] - Loss: 0.394325315952301\n",
            "Epoch [48/100] - Loss: 0.38895899057388306\n",
            "Epoch [49/100] - Loss: 0.3987486660480499\n",
            "Epoch [50/100] - Loss: 0.39341801404953003\n",
            "Epoch [51/100] - Loss: 0.3848840296268463\n",
            "Epoch [52/100] - Loss: 0.41333839297294617\n",
            "Epoch [53/100] - Loss: 0.3964543342590332\n",
            "Epoch [54/100] - Loss: 0.38686931133270264\n",
            "Epoch [55/100] - Loss: 0.3834056556224823\n",
            "Epoch [56/100] - Loss: 0.40672850608825684\n",
            "Epoch [57/100] - Loss: 0.3777250349521637\n",
            "Epoch [58/100] - Loss: 0.40473297238349915\n",
            "Epoch [59/100] - Loss: 0.4001601040363312\n",
            "Epoch [60/100] - Loss: 0.3729172945022583\n",
            "Epoch [61/100] - Loss: 0.3801273703575134\n",
            "Epoch [62/100] - Loss: 0.3988780379295349\n",
            "Epoch [63/100] - Loss: 0.38983431458473206\n",
            "Epoch [64/100] - Loss: 0.4011339247226715\n",
            "Epoch [65/100] - Loss: 0.37703314423561096\n",
            "Epoch [66/100] - Loss: 0.39436349272727966\n",
            "Epoch [67/100] - Loss: 0.39779189229011536\n",
            "Epoch [68/100] - Loss: 0.3815407454967499\n",
            "Epoch [69/100] - Loss: 0.3799905478954315\n",
            "Epoch [70/100] - Loss: 0.37314388155937195\n",
            "Epoch [71/100] - Loss: 0.3848715126514435\n",
            "Epoch [72/100] - Loss: 0.38385990262031555\n",
            "Epoch [73/100] - Loss: 0.40339064598083496\n",
            "Epoch [74/100] - Loss: 0.38801053166389465\n",
            "Epoch [75/100] - Loss: 0.3796171247959137\n",
            "Epoch [76/100] - Loss: 0.3889016807079315\n",
            "Epoch [77/100] - Loss: 0.37775281071662903\n",
            "Epoch [78/100] - Loss: 0.3950153589248657\n",
            "Epoch [79/100] - Loss: 0.3756203353404999\n",
            "Epoch [80/100] - Loss: 0.39039352536201477\n",
            "Epoch [81/100] - Loss: 0.40080443024635315\n",
            "Epoch [82/100] - Loss: 0.37811943888664246\n",
            "Epoch [83/100] - Loss: 0.36997729539871216\n",
            "Epoch [84/100] - Loss: 0.3653618097305298\n",
            "Epoch [85/100] - Loss: 0.37681645154953003\n",
            "Epoch [86/100] - Loss: 0.3837664723396301\n",
            "Epoch [87/100] - Loss: 0.3672192692756653\n",
            "Epoch [88/100] - Loss: 0.39031749963760376\n",
            "Epoch [89/100] - Loss: 0.3621864318847656\n",
            "Epoch [90/100] - Loss: 0.3923356533050537\n",
            "Epoch [91/100] - Loss: 0.38070592284202576\n",
            "Epoch [92/100] - Loss: 0.39401042461395264\n",
            "Epoch [93/100] - Loss: 0.380251944065094\n",
            "Epoch [94/100] - Loss: 0.3820444643497467\n",
            "Epoch [95/100] - Loss: 0.37689000368118286\n",
            "Epoch [96/100] - Loss: 0.37916553020477295\n",
            "Epoch [97/100] - Loss: 0.37338632345199585\n",
            "Epoch [98/100] - Loss: 0.3491721749305725\n",
            "Epoch [99/100] - Loss: 0.377212792634964\n",
            "Epoch [100/100] - Loss: 0.38179638981819153\n",
            "Anomalous Nodes: tensor([], size=(0, 2), dtype=torch.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Start Federated Learning (FL)\n",
        "# start_federated_learning()  # Federated Learning Setup\n",
        "# print(\"Federated Learning done successfully!\")\n",
        "from threading import Thread\n",
        "Thread(target=start_federated_learning).start()\n",
        "\n",
        "# Step 7: Proceed to RL Agent Training (you can move ahead with RL as before)\n",
        "# rl_model = train_rl_agent()\n",
        "# print(\"RL Agent trained successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohQlWBUJzxsM",
        "outputId": "afb4fa5a-5093-4073-c635-08aa0be277f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
            "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
            "\n",
            "\t\t$ flower-superlink --insecure\n",
            "\n",
            "\tTo view usage and all available options, run:\n",
            "\n",
            "\t\t$ flower-superlink --help\n",
            "\n",
            "\tUsing `start_server()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
            "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
            "\n",
            "\t\t$ flower-superlink --insecure\n",
            "\n",
            "\tTo view usage and all available options, run:\n",
            "\n",
            "\t\t$ flower-superlink --help\n",
            "\n",
            "\tUsing `start_server()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok flwr torch --quiet\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8080, \"tcp\")\n",
        "print(f\"Server accessible at: {public_url}\")\n",
        "\n",
        "# Modified server code\n",
        "def start_server():\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        min_fit_clients=2,\n",
        "        min_available_clients=2\n",
        "    )\n",
        "    fl.server.start_server(\n",
        "        server_address=\"0.0.0.0:8080\",\n",
        "        config={\"num_rounds\": 3},\n",
        "        strategy=strategy\n",
        "    )\n",
        "\n",
        "# Run in background\n",
        "import threading\n",
        "server_thread = threading.Thread(target=start_server)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "gwWeE98t0Hvf",
        "outputId": "582f4b5e-d1af-4bef-ac1e-b3d0035b7dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-03-13T00:52:20+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-03-13T00:52:20+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-03-13T00:52:20+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-03-13T00:52:20+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-3dc924973236>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Start ngrok tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8080\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tcp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Server accessible at: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    429\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Server (run first)\n",
        "def start_server():\n",
        "    fl.server.start_server(\n",
        "        server_address=\"0.0.0.0:8080\",\n",
        "        config={\"num_rounds\": 3},\n",
        "        strategy=fl.server.strategy.FedAvg(\n",
        "            min_fit_clients=2,\n",
        "            min_available_clients=2\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Client (run in separate cells)\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, trainloader):\n",
        "        self.model = model\n",
        "        self.trainloader = trainloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_parameters(parameters)\n",
        "        # Add training logic\n",
        "        return self.get_parameters(config), len(self.trainloader), {}\n",
        "\n",
        "# Client connection (run in separate cells)\n",
        "def client_fn(cid: str) -> fl.client.Client:\n",
        "    model = Net()  # Your model class\n",
        "    trainloader = ...  # Your data\n",
        "    return FlowerClient(model, trainloader).to_client()\n",
        "\n",
        "# Start multiple clients\n",
        "for i in range(2):\n",
        "    client_thread = threading.Thread(target=client_fn, args=(str(i),))\n",
        "    client_thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esprBB7U0Klb",
        "outputId": "0cf4f2f3-3f03-464c-db6e-ca78adb1d672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-65 (client_fn):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-56-d164ee370dd6>\", line 30, in client_fn\n",
            "NameError: name 'Net' is not defined. Did you mean: 'set'?\n",
            "Exception in thread Thread-66 (client_fn):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-56-d164ee370dd6>\", line 30, in client_fn\n",
            "NameError: name 'Net' is not defined. Did you mean: 'set'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SkLX_bvOvXBi",
        "outputId": "19d9d6cd-1ea0-425b-b678-be39f5a539af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable_baselines3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.1\n",
            "    Uninstalling gymnasium-1.1.1:\n",
            "      Successfully uninstalled gymnasium-1.1.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gymnasium-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gymnasium"
                ]
              },
              "id": "8056443c890e494aa9d2955ff026089d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shimmy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-hYW7JWwVje",
        "outputId": "94223878-f3b5-4ad7-87b4-3b66ff2a0191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o291CXJN1m3H",
        "outputId": "ac2f17b4-7702-4676-d26c-2e15615c71bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ORWjoL3dwJ",
        "outputId": "f9dcd7de-e52d-47ed-d909-3224b24f8fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhY7gEE08mBI",
        "outputId": "ba2be8af-e14d-451e-b8a7-9feca4a8ea30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flwr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32CnsTC5SHv3",
        "outputId": "c23d0834-e613-4fd9-f66e-f103ae88118c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr\n",
            "  Downloading flwr-1.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting cryptography<45.0.0,>=44.0.1 (from flwr)\n",
            "  Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.70.0)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.26.4)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.25.6)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Downloading flwr-1.16.0-py3-none-any.whl (532 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m532.1/532.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli-w, tomli, pycryptodome, pathspec, iterators, cryptography, typer, flwr\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.2 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cryptography-44.0.2 flwr-1.16.0 iterators-0.0.2 pathspec-0.12.1 pycryptodome-3.21.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjJrlx-tS_1k",
        "outputId": "757d1e21-754f-48a6-f11e-5a2c72122ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.5.0\n"
          ]
        }
      ]
    }
  ]
}